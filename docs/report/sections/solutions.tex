\subsection{Privacy-Preserving Techniques}

Given the risks outlined in Section~3, several approaches aim to reduce unnecessary data collection and limit exposure of sensitive information. Federated learning and other forms of on-device personalization train parts of the model directly on user devices, sending only aggregated updates rather than raw behavioural data. This substantially reduces the amount of identifiable information transmitted to central servers and is recognized as a promising privacy-preserving strategy \cite{ojokoh2025privacy}.

User-facing controls further strengthen privacy and autonomy. Options to view, modify, or delete stored data—and to disable personalized recommendations—give users clearer control over what is collected. Lightweight explanation tools, such as “Why am I seeing this?” prompts, help users understand how their past interactions influence recommendations. Together, these measures reduce unnecessary data exposure while improving transparency around system behaviour.

\subsection{Fairness, Explainability, and System Accountability}

Beyond privacy, improving the societal impact of recommender systems requires shifting optimization goals away from maximizing engagement and toward fairness, well-being, and accountability. Fairness must be treated as a multi-dimensional objective, jointly considered with diversity and novelty, to ensure more balanced exposure across users and creators \cite{deldjoo2023fairness}. Incorporating such objectives helps move systems from keeping users online toward serving them well and supporting equitable creator visibility.

Well-being based optimization can also reduce harmful feedback loops. Systems can detect signs of emotional or ideological narrowing, such as distressing content cycles or strong topical repetition, and adjust recommendations accordingly. Liu et~al.\ (2025) argue that large-scale recommenders “must go beyond personalization to support responsible consumption and foster social good” \cite{rssocialgood2025}.

Finally, accountability frameworks provide essential oversight. Independent audits of exposure patterns, fairness metrics, and potential emotional or ideological impacts can reveal systemic biases. As Liu et~al.\ (2025) \cite{rssocialgood2025} notes, accountability and reporting mechanisms increase transparency and help build public trust. These strategies collectively promote recommender systems that prioritize equitable treatment and long-term user well-being.